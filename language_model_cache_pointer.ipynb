{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes with [this blog post](https://sgugger.github.io/pointer-cache-for-language-model.html#pointer-cache-for-language-model) that explains what the continuous cache pointer is. This technique was introduce by Grave et al. in [this article](https://arxiv.org/pdf/1612.04426.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dill as pickle\n",
    "import json\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from hazm import *\n",
    "from fastai.text import *\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = 'extract/AA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on how I created the train and validation indexes look at this [jupyter notebook](https://github.com/layla-tadjpour/Deep-Learning/blob/master/language_model_persian.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LM_PATH=Path('extract/AA/persian_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids_hazm.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids_hazm.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos_hazm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60002, 188257)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60002, 5046)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = collections.defaultdict(lambda: 0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos),stoi['سلام']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model I use as en example is stored here. Be sure to have the file best.h5 in a directory called models where the variable PATH points to (our replace by any model you've saved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.load('lm_hazm_ft_after_32epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by computing how well our model is doing before anything else. To do that we will need a way to go through all of our text, but instead of using the fastai LanguageModelLoader (who randomly modifies the bptt) we'll change the code to have a fixed bptt.\n",
    "\n",
    "Also we don't want to do mini-batches on this validation because it resets the hidden state at each batch, making us lose valuable information. It makes a tiny bit of difference as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comes from the LanguageModelLoader class, I just removed the minibatch and fixed the bptt.\n",
    "#Now it gives an iterator that will spit bits of size bptt.\n",
    "class TextReader():\n",
    "    def __init__(self, nums, bptt, backwards=False):\n",
    "        self.bptt,self.backwards = bptt,backwards\n",
    "        self.data = self.batchify(nums)\n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            res = self.get_batch(self.i, self.bptt)\n",
    "            self.i += self.bptt\n",
    "            self.iter += 1\n",
    "            yield res\n",
    "\n",
    "    def __len__(self): return self.n // self.bptt \n",
    "\n",
    "    def batchify(self, data):\n",
    "        data = np.array(data)[:,None]\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return T(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source = self.data\n",
    "        seq_len = min(seq_len, len(source) - 1 - i)\n",
    "        return source[i:i+seq_len], source[i+1:i+1+seq_len].view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TextReader will give us an iterator that will allow us to go through the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_validate(model, source, bptt=2000):\n",
    "    data_source = TextReader(source, bptt)\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    for inputs, targets in tqdm(data_source):\n",
    "        #The language model throws up a bucnh of things, we'll focus on that later. For now we just want the ouputs.\n",
    "        outputs, raws, outs = model(V(inputs))\n",
    "        #The output doesn't go through softmax so we can use the CrossEntropy loss directly \n",
    "        total_loss += F.cross_entropy(outputs, V(targets), size_average=False).data[0]\n",
    "    #Total size is length of our iterator times bptt\n",
    "    mean = total_loss / (bptt * len(data_source))\n",
    "    #Returns loss and perplexity.\n",
    "    return mean, np.exp(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49052,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204595/204595 [49:07<00:00, 69.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.043599833351682, 57.03127681238258)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_validate(learner.model, np.concatenate(val_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(vec, size=vs, cuda=True):\n",
    "    a = torch.zeros(len(vec), size)\n",
    "    for i,v in enumerate(vec):\n",
    "        a[i,v] = 1.\n",
    "    return V(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_cache_pointer(model, source, theta = 0.662, lambd = 0.1279, window=200, bptt=2000):\n",
    "    data_source = TextReader(source, bptt)\n",
    "    #Set the model into eval mode.\n",
    "    model.eval()\n",
    "    #Just to create a hidden state.\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    #Containers for the previous targets/hidden states.\n",
    "    targ_history = None\n",
    "    hid_history = None\n",
    "    for inputs, targets in tqdm(data_source):\n",
    "        outputs, raws, outs = model(V(inputs))\n",
    "        #The outputs aren't softmaxed, sowe have to do it to get the p_vocab vectors.\n",
    "        p_vocab = F.softmax(outputs)\n",
    "        #We take the last hidden states (raws contains one Tensor for the results of each layer) and remove the batch dimension.\n",
    "        hiddens = raws[-1].squeeze() \n",
    "        #Start index inside our history.\n",
    "        start = 0 if targ_history is None else targ_history.size(0)\n",
    "        #Add the targets and hidden states to our history.\n",
    "        targ_history = one_hot(targets) if targ_history is None else torch.cat([targ_history, one_hot(targets)])\n",
    "        hid_history = hiddens if hid_history is None else torch.cat([hid_history, hiddens])\n",
    "        for i, pv in enumerate(p_vocab):\n",
    "            #Get the cached values\n",
    "            p = pv\n",
    "            if start + i > 0:\n",
    "                targ_cache = targ_history[:start+i] if start + i <= window else targ_history[start+i-window:start+i]\n",
    "                hid_cache = hid_history[:start+i] if start + i <= window else hid_history[start+i-window:start+i]\n",
    "                #This is explained in the blog post.\n",
    "                all_dot_prods = torch.mv(theta * hid_cache, hiddens[i])\n",
    "                softmaxed = F.softmax(all_dot_prods).unsqueeze(1)\n",
    "                p_cache = (softmaxed.expand_as(targ_cache) * targ_cache).sum(0).squeeze()\n",
    "                p = (1-lambd) * pv + lambd * p_cache\n",
    "            total_loss -= torch.log(p[targets[i]]).data[0]\n",
    "        targ_history = targ_history[-window:]\n",
    "        hid_history = hid_history[-window:]\n",
    "    #Total size is length of our iterator times bptt\n",
    "    mean = total_loss / (bptt * len(data_source))\n",
    "    #Returns loss and perplexity\n",
    "    return mean, np.exp(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7160/7160 [3:27:16<00:00,  1.74s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.9760679426004084, 53.30701535078493)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cache_pointer(learner.model, np.concatenate(val_lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we went from 57.03 perplexity to 53.30. This result can be imporved by increasing the window length, though it would take longer to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model to predict next words in farsi for texts of various lenghts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_str(s): return Tokenizer().spacy_tok(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_str(s): \n",
    "    idx_arr = np.array([stoi[tok] for tok in proc_str(s)])\n",
    "    return torch.from_numpy(np.expand_dims(idx_arr,axis=1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_model(m, s, l=50):\n",
    "    t = num_str(s)\n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(Variable(t))\n",
    "    print('...', end='')\n",
    "\n",
    "    for i in range(l):\n",
    "        n=res[-1].topk(2)[1]\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        word = itos[n.data[0]]\n",
    "        print(word, end=' ')\n",
    "        #if word=='<eos>': break\n",
    "        res,*_ = m(n[0].unsqueeze(0))\n",
    "\n",
    "    m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = \"\"\"\n",
    "\n",
    "به نظرم توجه به رضا شاه نوعی نومیدی از شرایطی است\n",
    "که نهاد دولت را از اقتدار تهی کرده است به طوری که توان\n",
    "تصمیم گیری برای حل معضلات کشور و یا اجرای تصمیمات خود را \n",
    "ندارد و کارش به \"حرف درمانی\" تقلیل یافته است. \n",
    "در واقع با کمی تأمل می توان دریافت که کشور نه فقط دستخوش نوعی\n",
    "از ملوک الطوایفی است و مسئولان هر استانی ساز خود را می نوازند،\n",
    "بلکه در هر شهر و آبادی نیز دهها نهاد و دستگاه با تعریف منافع\n",
    "و رانت های مشخص اقتصادی و سیاسی و مدیریتی برای خود و اطرافیان شان، در جهت خنثی ساز فعالیت های یکدیگر در نزاع و رقابت اند و از این جهت نه فقط مشکلی را حل نمی کنند بلکه به انباشت روزافزون مشکلات دامن می زنند.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...در این زمینه می گوید که در این دوره از تاریخ ایران ، ایران به عنوان یک کشور مستقل و مستقل از ایران و جهان شناخته می شود و در این زمینه به عنوان یک کشور مستقل و مستقل در نظر گرفته_می‌شود که در آن کشور به عنوان یک کشور "
     ]
    }
   ],
   "source": [
    "sample_model(m,ss,l=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss= \"\"\"\n",
    "پیش از این هم نوشته بودم که در شرایط انباشت مشکلات،\n",
    "\n",
    "یک فرد عادی جامعه در پی دمکراسی و این قبیل سخنان نیست؛ او صرفاً می خواهد که مشکلاتش از هر طریقی حل شود. در چنین شرایطی، زمینه برای ظهور فرد مقتدر و با صلابت و حتی مستبدی فراهم می شود که کارآیی خود را در عمل نشان دهد و در نزاع بین کانون های متعدد قدرت، تصمیمی فیصله بخش بگیرد. چنین فردی اگر در حوزۀ زندگی خصوصی و فرهنگی و شیوۀ زیست افراد دخالت نکند و آنها را از این جهت به تنگ نیاورد و با دنیا هم به گونه ای راه سازش و مسالمت در پیش گیرد؛ می تواند از اقبال عمومی برخوردار شود.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...، ص در این میان ، در این میان ، در این میان ، در این میان ، در این میان ، در این میان ، به نظر می‌رسد که این دو ، در واقع ، به نوعی از این دو ، در کنار یکدیگر ، به هم پیوند خورده‌اند "
     ]
    }
   ],
   "source": [
    "sample_model(m,ss,l=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss= \"وای این فخرآور خیلی بانمکه که برای خودش رفته پول داده امضای دونالد ترامپ خریده و بعد به خودش از قول ترامپ گفته: توماس جفرسون ایران. می‌خوام برم یکی عینش رو سفارش بدم بگم دونالد برام بنویسه تو شهناز تهرانی آمریکایی.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...، زاده اکتبر درگذشته مارس کارآفرین ، بازرگان و مدیر ارشد اجرایی بریتانیایی بود ، که در سال شرکت خودروسازی "
     ]
    }
   ],
   "source": [
    "sample_model(m,ss,l=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
